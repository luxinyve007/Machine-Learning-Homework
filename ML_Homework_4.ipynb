{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#The-Perceptron-Classifier\" data-toc-modified-id=\"The-Perceptron-Classifier-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>The Perceptron Classifier</a></span></li><li><span><a href=\"#The-XOR_Gate-Class\" data-toc-modified-id=\"The-XOR_Gate-Class-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>The XOR_Gate Class</a></span></li><li><span><a href=\"#Generate-the-Features\" data-toc-modified-id=\"Generate-the-Features-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Generate the Features</a></span></li><li><span><a href=\"#Train-and-Test-the-XOR_Gate\" data-toc-modified-id=\"Train-and-Test-the-XOR_Gate-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Train and Test the XOR_Gate</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"5\">\n",
    "Machine Learning I (DATS 6202), Fall 2019\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"4\">\n",
    "Homework 4\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Data Science, Columbian College of Arts & Sciences, George Washington University\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Author: Yuxiao Huang\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- Your friend, E (an engineer), wants to simulate an XOR ($\\oplus$) gate. E googled it and found the solution on [wikipedia](https://en.wikipedia.org/wiki/XOR_gate). It turns out that, an XOR gate can be built using two logic AND ($\\wedge$) gates and one logic OR ($\\vee$) gate:\n",
    "\n",
    "$$x_1 \\oplus x_2 = \\big( x_1 \\wedge \\neg x_2 \\big) \\vee \\big( \\neg x_1 \\wedge x_2 \\big).$$\n",
    "    \n",
    "- Here, the XOR gate takes as binary input $x_1$ and $x_2$. The input of one AND gate is comprised of $x_1$ and $\\neg x_2$ (the first two items on the right-hand side of the equation above), and that of the other AND gate is composed of $\\neg x_1$ and $x_2$ (the last two items in the equation). The output of the two AND gates is the input of the OR gate.\n",
    "\n",
    "- Based on the equation above, the original problem (simulating an XOR gate) becomes simulating the AND and OR gates. E realized that this is the time to ask help from you, a data scientist. You looked at the problem and found that the AND and OR gates can be simulated using perceptrons (since both gates generate data that are linearly separable). The only challenge is that, you have not learned how to train a sequence of perceptrons yet (i.e., a Neural Network). You looked at the equation above for a while and suddenly had the \"Aha!\" moment. \"It is actually an extremely simple problem because we can train the perceptrons independently!\" you said to E, and started writing a fit function that finds the weights of the three perceptrons. You then went beyond that and wrote a check function that tests whether the weights make sense (so that they can generate the correct output, based on the flow discussed below the equation of XOR). After seeing the perfect predictions, E was convinced that this is the simulation he was looking for.\n",
    "\n",
    "- Complete the missing parts indicated by # Implement me\n",
    "- Particularly, the code should\n",
    "    - be bug-free (while the output produced by your solution being the same as the provided output does not necessarily mean your code is bug-free, it is very likely that there is a bug in your code when the two kinds of output are different)\n",
    "    - be commented\n",
    "- **Marks will be deducted if the above requirements (for the code) are not met**\n",
    "- Submit an ipynb file named homework_4.ipynb to [blackboard](https://blackboard.gwu.edu) folder /Assignments/Homework_4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reference: The code below is from exercise_5_solution.ipynb\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MyPerceptron(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"The perceptron classifier\"\"\"\n",
    "        \n",
    "    def __init__(self, n_iter=100, eta=0.01, random_state=0):\n",
    "        # The number of iterations\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "        # The learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # The random state\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # The cost\n",
    "        self.cost = None\n",
    "        \n",
    "        # The parameters\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        y : the target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the cost\n",
    "        self.cost = []\n",
    "        \n",
    "        # The random number generator\n",
    "        self.rgen = np.random.RandomState(seed=self.random_state)\n",
    "        \n",
    "        # Initialize the weight for features x0 (the dummy feature), x1, x2, ..., xn (with respect to each class)\n",
    "        # Here self.w is a X.shape[1] + 1 by len(self.classes) matrix\n",
    "        self.w = self.rgen.normal(loc=0.0, scale=0.01, size=(X.shape[1] + 1))\n",
    "\n",
    "        # For each iteration\n",
    "        for _ in range(self.n_iter):\n",
    "            # Implement me\n",
    "            # Get the net input\n",
    "            # Here net_input is a X.shape[0] by len(self.classes) matrix\n",
    "            net_input = \n",
    "            \n",
    "            # Implement me\n",
    "            # Get the activation\n",
    "            # Here activation is a X.shape[0] by len(self.classes) matrix\n",
    "            activation = \n",
    "            \n",
    "            # Implement me\n",
    "            # Get the errors\n",
    "            # Here errors is a X.shape[0] by len(self.classes) matrix     \n",
    "            errors = \n",
    "            \n",
    "            # Get the mean squared error (mse)\n",
    "            mse = (errors ** 2).sum()\n",
    "            \n",
    "            # Update the cost\n",
    "            self.cost.append(mse)\n",
    "\n",
    "            # Implement me\n",
    "            # Update the weights for features x1, x2, ..., xn\n",
    "            self.w[1:] += \n",
    "\n",
    "            # Implement me\n",
    "            # Update the weights for the dummy feature, x0\n",
    "            self.w[0] += \n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"\n",
    "        Get the net input\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The net input\n",
    "       \n",
    "        \"\"\"\n",
    "        \n",
    "        # Implement me\n",
    "    \n",
    "    def activation(self, net_input):\n",
    "        \"\"\"\n",
    "        Here we use the hard limit function as the activation function:\n",
    "        activation = 1, if net_input >=0\n",
    "        activation = 0, otherwise\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        net_input : the net input\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The activation      \n",
    "        \"\"\"\n",
    "\n",
    "        # Implement me\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted class labels of the target\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement me\n",
    "        # Get the net input (using the net_input function defined above)\n",
    "        net_input = \n",
    "\n",
    "        # Implement me\n",
    "        # Get the activation (using the activation function defined above)\n",
    "        activation = \n",
    "            \n",
    "        return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The XOR_Gate Class\n",
    "1. You should use the perceptron classifier above (implemented in exercise_5)\n",
    "2. You cannot use any other classifiers, they are not what your friend E wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class XOR_Gate():\n",
    "    \"\"\"The XOR_Gate classifier\"\"\"\n",
    "      \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the gates dictionary\"\"\"\n",
    "        self.gates = {}\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        X : the feature matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate data for gates 1 to 3\n",
    "        X1, y1, X2, y2, X3, y3 = self.get_X123_y123(X)\n",
    "\n",
    "        # Implement me\n",
    "        # Fit the logic and gate: y1 = (x1) and (not x2)\n",
    "        self.gates[1] = \n",
    "        self.gates[1].fit(, )\n",
    "        # Print the weights of the gate\n",
    "        self.print_weights(1)\n",
    "\n",
    "        # Implement me\n",
    "        # Fit the logic and gate: y2 = (not x1) and (x2)\n",
    "        self.gates[2] = \n",
    "        self.gates[2].fit(, )\n",
    "        # Print the weights of the gate\n",
    "        self.print_weights(2)\n",
    "\n",
    "        # Implement me\n",
    "        # Fit the logic or gate: y3 = (y1) or (y2) \n",
    "        self.gates[3] = \n",
    "        self.gates[3].fit(, )\n",
    "        # Print the weights of the gate\n",
    "        self.print_weights(3)\n",
    "        \n",
    "    def print_weights(self, i):\n",
    "        \"\"\"\n",
    "        Print the weights of gate i\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        i : the number of the gate\n",
    "        \"\"\"\n",
    "        \n",
    "        print('------------------------')\n",
    "        print(\"Weights for gate \" + str(i) + ':')\n",
    "        print('w0: ', round(self.gates[i].w[0], 2))\n",
    "        print('w1: ', round(self.gates[i].w[1], 2))\n",
    "        print('w2: ', round(self.gates[i].w[2], 2))\n",
    "        print()\n",
    "        \n",
    "    def get_X123_y123(self, X):\n",
    "        \"\"\"\n",
    "        Generate data for gates 1 to 3.\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        X : the feature matrix\n",
    "          \n",
    "        Returns\n",
    "        ------------\n",
    "        The data generated for gates 1 to 3\n",
    "        \"\"\"\n",
    "        \n",
    "        # Data for logic and gate: y1 = (x1) and (not x2)\n",
    "        X1 = np.hstack((X[:, 0].reshape(-1, 1), np.where(X[:, 1], 0, 1).reshape(-1, 1)))\n",
    "        y1 = np.logical_and(X1[:, 0], X1[:, 1])\n",
    "        y1 = np.where(y1, 1, 0)\n",
    "\n",
    "        # Data for logic and gate: y2 = (not x1) and (x2)\n",
    "        X2 = np.hstack((np.where(X[:, 0], 0, 1).reshape(-1, 1), X[:, 1].reshape(-1, 1)))\n",
    "        y2 = np.logical_and(X2[:, 0], X2[:, 1])\n",
    "        y2 = np.where(y2, 1, 0)\n",
    "\n",
    "        # Data for logic or gate: y3 = (y1) or (y2) \n",
    "        X3 = np.hstack((y1.reshape(-1, 1), y2.reshape(-1, 1)))\n",
    "        y3 = np.logical_or(X3[:, 0], X3[:, 1])\n",
    "        y3 = np.where(y3, 1, 0)\n",
    "        \n",
    "        return [X1, y1, X2, y2, X3, y3]\n",
    "        \n",
    "    def check(self, X):\n",
    "        \"\"\"\n",
    "        Calculate number of wrong predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        X: the feature matrix\n",
    "        \"\"\"\n",
    "           \n",
    "        # Generate data for gates 1 to 3\n",
    "        X1, y1, X2, y2, X3, y3 = self.get_X123_y123(X)\n",
    "        \n",
    "        # Implement me\n",
    "        # Predict the value using the logic and gate: y1 = (x1) and (not x2)\n",
    "        y1_pred = \n",
    "        \n",
    "        # Implement me\n",
    "        # Predict the value using the logic and gate: y2 = (not x1) and (x2)\n",
    "        y2_pred = \n",
    "\n",
    "        # Implement me\n",
    "        # Create input for gate 3, X3_pred, from y1_pred and y2_pred\n",
    "        # Hint: See function get_X123_y123 for ideas\n",
    "        X3_pred = \n",
    "        # Predict the value using the logic or gate: y3 = (y1) or (y2)\n",
    "        y3_pred = self.gates[3].predict(X3_pred)\n",
    "        \n",
    "        # Print precision, recall, fscore, and support\n",
    "        print('------------------------')\n",
    "        print('The nubmer of wrong predictions:')\n",
    "        print((y3 - y3_pred).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "X_train = np.random.randint(2, size=(100, 2))\n",
    "X_test = np.random.randint(2, size=(100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test the XOR_Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Weights for gate 1:\n",
      "w0:  -0.26\n",
      "w1:  0.2\n",
      "w2:  0.22\n",
      "\n",
      "------------------------\n",
      "Weights for gate 2:\n",
      "w0:  -0.48\n",
      "w1:  0.3\n",
      "w2:  0.29\n",
      "\n",
      "------------------------\n",
      "Weights for gate 3:\n",
      "w0:  -0.08\n",
      "w1:  0.24\n",
      "w2:  0.22\n",
      "\n",
      "------------------------\n",
      "The nubmer of wrong predictions:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Declare the XOR_Gate classifier\n",
    "xor_gate = XOR_Gate()\n",
    "\n",
    "# Train the classifier\n",
    "xor_gate.fit(X_train)\n",
    "\n",
    "# Test the classifier\n",
    "xor_gate.check(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
